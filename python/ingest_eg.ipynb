{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a01d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tiledb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f1b92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lums/TileDB/feature-vector-prototype/python\n"
     ]
    }
   ],
   "source": [
    "os.system('pwd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03c43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = tiledb.Ctx().config()\n",
    "\n",
    "# cfg[\"py.init_buffer_bytes\"] = 1024**2 * 50\n",
    "# cfg[\"vfs.s3.scheme\"] = \"https\" \n",
    "# cfg[\"vfs.s3.region\"] = \"us-west-2\"\n",
    "# cfg[\"vfs.s3.endpoint_override\"] = \"\"\n",
    "# cfg[\"vfs.s3.use_virtual_addressing\"] = \"true\"\n",
    "# cfg[\"vfs.s3.aws_access_key_id\"] = \"AKIA2HZNSCDDICHRA6P2\";\n",
    "# cfg[\"vfs.s3.aws_secret_access_key\"] = \"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\";\n",
    "\n",
    "tiledb.default_ctx({\"vfs.s3.region\": \"us-west-2\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f7662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_info(filename):\n",
    "    # Check if filename exists\n",
    "    if (not os.path.exists(filename)):\n",
    "        raise Exception(f'{filename} does not exist.')\n",
    "\n",
    "    file_size = os.path.getsize(filename)\n",
    "    print(f'The size of {filename} is {file_size} bytes.')\n",
    "\n",
    "    f = open(filename, 'rb')\n",
    "    dimension = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "    num_vectors = file_size // (4 + dimension * 4)   # Four bytes for float or int\n",
    "\n",
    "    print(f'num_vectors is {num_vectors}, dimension is {dimension}')\n",
    "    f.close()\n",
    "    \n",
    "    return num_vectors, dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61243099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fvecs_generator(filename, num_vectors, dimension, block_size):\n",
    "\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    if (ext == '.fvecs'):\n",
    "        element_type = np.float32\n",
    "    elif (ext == '.ivecs'):\n",
    "        element_type = np.int32\n",
    "    elif (ext == '.bvecs'):\n",
    "        element_type = np.uint8\n",
    "    else:\n",
    "        raise Exception(f'Unkown extension {ext}')\n",
    "    \n",
    "    f = open(filename, 'rb')\n",
    "    \n",
    "    num_vectors_read = 0\n",
    "    chunk_size = block_size * (dimension + 1)  # number of vectors by number of elements / vector\n",
    "    print(f'In generator chunk_size is {chunk_size}, block_size is {block_size}, dimension is {dimension}')\n",
    "    while True:\n",
    "        count = block_size\n",
    "        if ((num_vectors_read + block_size) > num_vectors):\n",
    "            count = (num_vectors_read + block_size) - num_vectors\n",
    "\n",
    "        chunk_size = count * (dimension + 1)\n",
    "        # chunk_size is number of elements\n",
    "        # if num_vectors // block_size == 0\n",
    "        # remaining vectors is num_vectors % block_size\n",
    "\n",
    "        chunk = np.fromfile(f, dtype=element_type, count=chunk_size)\n",
    "        if chunk.size == 0:\n",
    "            if (num_vectors_read != num_vectors):\n",
    "                raise Exception(f'{num_vectors_read} != {num_vectors}')\n",
    "            break\n",
    "        \n",
    "        # print(f'before reshape chunk is {type(chunk)} and size {chunk.shape}')\n",
    "        b = chunk.reshape(count, dimension+1)\n",
    "        b = np.transpose(b[:, 1:dimension+1])\n",
    "            \n",
    "        num_vectors_read = num_vectors_read + block_size\n",
    "\n",
    "        # print(f'after reshape chunk is {type(b)} and size {b.shape}')\n",
    "\n",
    "        yield b\n",
    "    print(f'total read {num_vectors_read}'')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff560ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array(array_name, num_vectors, dimension, tile_size):\n",
    "    \n",
    "    print(f'Creating array {array_name}: {dimension} by {num_vectors} with tile_size {tile_size}')\n",
    "    \n",
    "    # The array will be dimension by num_vectors                                                  \n",
    "    dom = tiledb.Domain(\n",
    "        tiledb.Dim(name=\"rows\", domain=(0, dimension-1), tile=dimension, dtype=np.int32),\n",
    "        tiledb.Dim(name=\"cols\", domain=(0, num_vectors-1), tile=tile_size, dtype=np.int32),\n",
    "    )\n",
    "\n",
    "    # The array will be dense with a single attribute \"a\" so each (i,j) cell can store a float.                                  \n",
    "    schema = tiledb.ArraySchema(\n",
    "        domain=dom, sparse=False, attrs=[tiledb.Attr(name=\"a\", dtype=np.float32)], \n",
    "        cell_order='col-major', tile_order='col-major'\n",
    "    )\n",
    "\n",
    "    # Create the (empty) array on disk.   \n",
    "    if (tiledb.object_type(array_name) == \"array\"):\n",
    "        print(f\"Array {array_name} already exists.  Deleting\")\n",
    "        tiledb.remove(array_name)\n",
    "    tiledb.DenseArray.create(array_name, schema)\n",
    "    \n",
    "    # Check size\n",
    "    # with tiledb.DenseArray(array_name, mode=\"r\") as A:\n",
    "    #    data = A[:]\n",
    "    #    print(f'After creation, array shape is {data[\"a\"].shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8be3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_array(array_name, num_vectors, dimension, block_size):\n",
    "\n",
    "    print(f'Opening array {array_name} for writing with num_vectors {num_vectors} and dimension {dimension}')\n",
    "    \n",
    "    # Open the TileDB array for writing\n",
    "    A = tiledb.DenseArray(array_name, mode='w')\n",
    "\n",
    "    # Read data blocks from the file and write them to the TileDB array\n",
    "    generator = fvecs_generator(filename, num_vectors, dimension, block_size)\n",
    "\n",
    "    begin = 0\n",
    "    for block in generator:\n",
    "        # print(f'Read block number {begin} : block is {type(block)} with shape {block.shape}')\n",
    "        A[0:dimension, begin*block_size:(begin+1)*block_size] = block\n",
    "        begin = begin + 1\n",
    "        if begin*block_size >= num_vectors:\n",
    "            break\n",
    "\n",
    "    # Close the TileDB array\n",
    "    A.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ad85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 'sift_base'\n",
    "# array_name = 's3://tiledb-lums/sift_base'\n",
    "# array_name = 'tiledb://lums/sift_base'\n",
    "# array_name = 'https://tiledb-lums.s3.amazonaws.com/sift_base'\n",
    "# array_name = 'https://tiledb-lums.s3-us-west-2.amazonaws.com/sift_base'\n",
    "\n",
    "const_block_size = 10000\n",
    "# const_dim = 128\n",
    "tile_size = const_block_size\n",
    "# tile_size = const_dim\n",
    "\n",
    "\n",
    "# filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 's3://tiledb-lums/sift_base'\n",
    "\n",
    "\n",
    "filebase = '/Users/lums/TileDB/feature-vector-prototype/external/data'\n",
    "array_base = 's3://tiledb-lums'\n",
    "\n",
    "files = ['siftsmall/siftsmall_base.fvecs',\n",
    "        'siftsmall/siftsmall_groundtruth.ivecs',\n",
    "        'siftsmall/siftsmall_learn.fvecs',\n",
    "        'siftsmall/siftsmall_query.fvecs',\n",
    "        'sift/sift_base.fvecs',\n",
    "        'sift/sift_groundtruth.ivecs',\n",
    "        'sift/sift_learn.fvecs',\n",
    "        'sift/sift_query.fvecs']\n",
    "\n",
    "arrays= ['siftsmall_base',\n",
    "        'siftsmall_groundtruth',\n",
    "        'siftsmall_learn',\n",
    "        'siftsmall_query',\n",
    "        'sift_base',\n",
    "        'sift_groundtruth',\n",
    "        'sift_learn',\n",
    "        'sift_query']\n",
    "\n",
    "for f, a in zip(files, arrays):\n",
    "    filename = os.path.join(filebase, f)\n",
    "    array_name = os.path.join(array_base, a)\n",
    "    print(f'{filename} -> {array_name}')\n",
    "\n",
    "    num_vectors, dimension = get_data_info(filename)\n",
    "    tile_size = min(num_vectors, const_block_size)\n",
    "    block_size = min(num_vectors, const_block_size)\n",
    "    create_array(array_name, num_vectors, dimension, tile_size)\n",
    "    write_array(array_name, num_vectors, dimension, block_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10658345",
   "metadata": {},
   "source": [
    "### For bigann compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caad7f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f23fcbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_bvecs_gz(filename):\n",
    "    # Check if filename exists\n",
    "    if (not os.path.exists(filename)):\n",
    "        raise Exception(f'{filename} does not exist.')\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    if (ext != '.gz'):\n",
    "        raise Exception(f'{filename} is not a gz file')\n",
    "    \n",
    "    base, ext = os.path.splitext(base)\n",
    "    if (ext != '.bvecs'):\n",
    "        raise Exception(f'{filename} is not a bvecs file')\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a5414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gz_data_info(filename):\n",
    "    \n",
    "    # Check if filename exists\n",
    "    if (not is_valid_bvecs_gz(filename)):\n",
    "        raise Exception(f'Invalid file {filename}')\n",
    "\n",
    "    file_size = os.path.getsize(filename)\n",
    "    print(f'The size of the compressed {filename} is {file_size} bytes.')\n",
    "\n",
    "    # bigann_query.bvecs.gz\n",
    "    uncompressed_chunk_size = 4   # one int32\n",
    "\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        with io.BufferedReader(f, buffer_size=uncompressed_chunk_size) as reader:\n",
    "            bytes = reader.read(uncompressed_chunk_size)\n",
    "            if not bytes:\n",
    "                raise Exception('Could not get dimension size')\n",
    "            # Process chunk of uncompressed data\n",
    "            print(f'Read {len(bytes)} bytes')\n",
    "\n",
    "            dimension = np.frombuffer(bytes, dtype=np.int32, count=1)[0]\n",
    "            print(f'dimension is {dimension}')\n",
    "       \n",
    "    f.close()\n",
    "    \n",
    "    return None, dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28f604cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bvecs_gz_generator(filename, num_vectors, dimension, block_size):\n",
    "    \n",
    "    # Check if filename exists\n",
    "    if (not is_valid_bvecs_gz(filename)):\n",
    "        raise Exception(f'Invalid file {filename}')\n",
    "    \n",
    "    element_type = np.uint8\n",
    "    \n",
    "    uncompressed_chunk_size = block_size * (dimension + 1)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        with io.BufferedReader(f, buffer_size=uncompressed_chunk_size) as reader:\n",
    "                \n",
    "            num_vectors_read = 0\n",
    "            \n",
    "            print(f'In generator uncompressed_chunk_size is {uncompressed_chunk_size}, block_size is {block_size}, dimension is {dimension}')\n",
    "            print(f'        num_vectors is {num_vectors}')\n",
    "            while True:\n",
    "                count = block_size\n",
    "                if ((num_vectors_read + block_size) > num_vectors):\n",
    "                    count = (num_vectors_read + block_size) - num_vectors\n",
    "\n",
    "                uncompressed_chunk_size = count * (dimension + 1)\n",
    "                bytes = reader.read(uncompressed_chunk_size * 4)\n",
    "                \n",
    "                print(f'In generator uncompressed_chunk_size is {uncompressed_chunk_size}, block_size is {block_size}, dimension is {dimension}')\n",
    "                print(f'    count is {count}, lenbytes is {len(bytes)}')\n",
    "                if (len(bytes) == 0):\n",
    "                    print(f'end of file len_bytes is {len(bytes)}')\n",
    "                    break\n",
    "                chunk = np.frombuffer(bytes, dtype=element_type, count=uncompressed_chunk_size)\n",
    "                \n",
    "                if chunk.size == 0:\n",
    "                    print(f'end of file')\n",
    "                    if (num_vectors_read != num_vectors):\n",
    "                        raise Exception(f'{num_vectors_read} != {num_vectors}')\n",
    "                    break\n",
    "                if (num_vectors_read == num_vectors):\n",
    "                    print(f'read to {num_vectors_read} == {num_vectors}')\n",
    "                    break\n",
    "        \n",
    "                # print(f'before reshape chunk is {type(chunk)} and size {chunk.shape}')\n",
    "                b = chunk.reshape(count, dimension+1)\n",
    "                b = np.transpose(b[:, 1:dimension+1])\n",
    "                    \n",
    "                num_vectors_read = num_vectors_read + block_size\n",
    "                \n",
    "                print(f'num_vectors_read is {num_vectors_read} of {num_vectors}')\n",
    "\n",
    "                # print(f'after reshape chunk is {type(b)} and size {b.shape}')\n",
    "\n",
    "                yield b\n",
    "       \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec49d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_gz_array(array_name, num_vectors, dimension, block_size):\n",
    "\n",
    "    print(f'Opening array {array_name} for writing with num_vectors {num_vectors} and dimension {dimension}')\n",
    "    \n",
    "    # Open the TileDB array for writing\n",
    "    A = tiledb.DenseArray(array_name, mode='w')\n",
    "\n",
    "    # Read data blocks from the file and write them to the TileDB array\n",
    "    generator = bvecs_gz_generator(filename, num_vectors, dimension, block_size)\n",
    "\n",
    "    begin = 0\n",
    "    for block in generator:\n",
    "        \n",
    "        print(f'Read block number {begin} : block is {type(block)} with shape {block.shape}')\n",
    "        \n",
    "        A[0:dimension, begin*block_size:(begin+1)*block_size] = block\n",
    "        begin = begin + 1\n",
    "        if begin*block_size >= num_vectors:\n",
    "            break\n",
    "\n",
    "    # Close the TileDB array\n",
    "    A.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d284b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lums/TileDB/feature-vector-prototype/external/data/bigann_query.bvecs.gz -> s3://tiledb-lums/sift/bigann_query\n",
      "The size of the compressed /Users/lums/TileDB/feature-vector-prototype/external/data/bigann_query.bvecs.gz is 986411 bytes.\n",
      "Read 4 bytes\n",
      "dimension is 128\n",
      "Creating array s3://tiledb-lums/sift/bigann_query: 128 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/bigann_query already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/bigann_query for writing with num_vectors 10000 and dimension 128\n",
      "In generator uncompressed_chunk_size is 1290000, block_size is 10000, dimension is 128\n",
      "        num_vectors is 10000\n",
      "In generator uncompressed_chunk_size is 1290000, block_size is 10000, dimension is 128\n",
      "    count is 10000, lenbytes is 1320000\n",
      "num_vectors_read is 10000 of 10000\n",
      "Read block number 0 : block is <class 'numpy.ndarray'> with shape (128, 10000)\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/bigann_learn.bvecs.gz -> s3://tiledb-lums/sift/bigann_learn\n",
      "The size of the compressed /Users/lums/TileDB/feature-vector-prototype/external/data/bigann_learn.bvecs.gz is 9746888942 bytes.\n",
      "Read 4 bytes\n",
      "dimension is 128\n",
      "Creating array s3://tiledb-lums/sift/bigann_learn: 128 by 100000000 with tile_size 1000000\n",
      "Array s3://tiledb-lums/sift/bigann_learn already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/bigann_learn for writing with num_vectors 100000000 and dimension 128\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "        num_vectors is 100000000\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 1000000 of 100000000\n",
      "Read block number 0 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 2000000 of 100000000\n",
      "Read block number 1 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 3000000 of 100000000\n",
      "Read block number 2 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 4000000 of 100000000\n",
      "Read block number 3 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 5000000 of 100000000\n",
      "Read block number 4 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 6000000 of 100000000\n",
      "Read block number 5 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 7000000 of 100000000\n",
      "Read block number 6 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 8000000 of 100000000\n",
      "Read block number 7 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 9000000 of 100000000\n",
      "Read block number 8 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 10000000 of 100000000\n",
      "Read block number 9 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 11000000 of 100000000\n",
      "Read block number 10 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 12000000 of 100000000\n",
      "Read block number 11 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 13000000 of 100000000\n",
      "Read block number 12 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 14000000 of 100000000\n",
      "Read block number 13 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 15000000 of 100000000\n",
      "Read block number 14 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 16000000 of 100000000\n",
      "Read block number 15 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 17000000 of 100000000\n",
      "Read block number 16 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 18000000 of 100000000\n",
      "Read block number 17 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 19000000 of 100000000\n",
      "Read block number 18 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 20000000 of 100000000\n",
      "Read block number 19 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 21000000 of 100000000\n",
      "Read block number 20 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 22000000 of 100000000\n",
      "Read block number 21 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 23000000 of 100000000\n",
      "Read block number 22 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 24000000 of 100000000\n",
      "Read block number 23 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 25000000 of 100000000\n",
      "Read block number 24 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 300000000\n",
      "num_vectors_read is 26000000 of 100000000\n",
      "Read block number 25 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 0\n",
      "end of file len_bytes is 0\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/bigann_base.bvecs.gz -> s3://tiledb-lums/sift/bigann_base\n",
      "The size of the compressed /Users/lums/TileDB/feature-vector-prototype/external/data/bigann_base.bvecs.gz is 97941899519 bytes.\n",
      "Read 4 bytes\n",
      "dimension is 128\n",
      "Creating array s3://tiledb-lums/sift/bigann_base: 128 by 1000000000 with tile_size 1000000\n",
      "Array s3://tiledb-lums/sift/bigann_base already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/bigann_base for writing with num_vectors 1000000000 and dimension 128\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "        num_vectors is 1000000000\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 1000000 of 1000000000\n",
      "Read block number 0 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 2000000 of 1000000000\n",
      "Read block number 1 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 3000000 of 1000000000\n",
      "Read block number 2 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 4000000 of 1000000000\n",
      "Read block number 3 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 5000000 of 1000000000\n",
      "Read block number 4 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 6000000 of 1000000000\n",
      "Read block number 5 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 7000000 of 1000000000\n",
      "Read block number 6 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 8000000 of 1000000000\n",
      "Read block number 7 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 9000000 of 1000000000\n",
      "Read block number 8 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 10000000 of 1000000000\n",
      "Read block number 9 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 11000000 of 1000000000\n",
      "Read block number 10 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 12000000 of 1000000000\n",
      "Read block number 11 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 13000000 of 1000000000\n",
      "Read block number 12 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 14000000 of 1000000000\n",
      "Read block number 13 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 15000000 of 1000000000\n",
      "Read block number 14 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 16000000 of 1000000000\n",
      "Read block number 15 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 17000000 of 1000000000\n",
      "Read block number 16 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 18000000 of 1000000000\n",
      "Read block number 17 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 19000000 of 1000000000\n",
      "Read block number 18 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 20000000 of 1000000000\n",
      "Read block number 19 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 21000000 of 1000000000\n",
      "Read block number 20 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 22000000 of 1000000000\n",
      "Read block number 21 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 23000000 of 1000000000\n",
      "Read block number 22 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 24000000 of 1000000000\n",
      "Read block number 23 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 25000000 of 1000000000\n",
      "Read block number 24 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n",
      "In generator uncompressed_chunk_size is 129000000, block_size is 1000000, dimension is 128\n",
      "    count is 1000000, lenbytes is 516000000\n",
      "num_vectors_read is 26000000 of 1000000000\n",
      "Read block number 25 : block is <class 'numpy.ndarray'> with shape (128, 1000000)\n"
     ]
    }
   ],
   "source": [
    "# filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 'sift_base'\n",
    "# array_name = 's3://tiledb-lums/sift_base'\n",
    "# array_name = 'tiledb://lums/sift_base'\n",
    "# array_name = 'https://tiledb-lums.s3.amazonaws.com/sift_base'\n",
    "# array_name = 'https://tiledb-lums.s3-us-west-2.amazonaws.com/sift_base'\n",
    "\n",
    "const_block_size = 1000000\n",
    "# const_dim = 128\n",
    "tile_size = const_block_size\n",
    "# tile_size = const_dim\n",
    "\n",
    "\n",
    "# filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 's3://tiledb-lums/sift_base'\n",
    "\n",
    "\n",
    "filebase = '/Users/lums/TileDB/feature-vector-prototype/external/data'\n",
    "array_base = 's3://tiledb-lums/sift'\n",
    "\n",
    "files = ['bigann_query.bvecs.gz',\n",
    "         'bigann_learn.bvecs.gz',\n",
    "         'bigann_base.bvecs.gz']\n",
    "\n",
    "arrays = ['bigann_query',\n",
    "          'bigann_learn',\n",
    "          'bigann_base']\n",
    "\n",
    "nums = [ 10000,\n",
    "         100000000,\n",
    "         1000000000]\n",
    "\n",
    "for f, a, num_vectors in zip(files, arrays, nums):\n",
    "    filename = os.path.join(filebase, f)\n",
    "    array_name = os.path.join(array_base, a)\n",
    "    print(f'{filename} -> {array_name}')\n",
    "\n",
    "    _, dimension = get_gz_data_info(filename)\n",
    "    tile_size = min(num_vectors, const_block_size)\n",
    "    block_size = min(num_vectors, const_block_size)\n",
    "    create_array(array_name, num_vectors, dimension, tile_size)\n",
    "    write_gz_array(array_name, num_vectors, dimension, block_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33c1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/bigann_base.bvecs.gz'\n",
    "a, b = get_gz_data(filename)\n",
    "print(f'dimension b is {b}')\n",
    "g = fvecs_gz_generator(filename, 10, b, 5)\n",
    "print(f'g is {type(g)}')\n",
    "j = 0\n",
    "for i in g:\n",
    "    print(f'i is {type(i)} with shape {i.shape}')\n",
    "    if (j == 5):\n",
    "        break\n",
    "    j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1accb714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a63372f",
   "metadata": {},
   "source": [
    "### Some prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20178479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#bigann_query.bvecs.gz\n",
    "uncompressed_chunk_size = 1024 * 1024 # 1 MB chunk size\n",
    "i = 0\n",
    "with gzip.open('/Users/lums/TileDB/feature-vector-prototype/external/data/bigann_base.bvecs.gz', 'rb') as f:\n",
    "    with io.BufferedReader(f, buffer_size=uncompressed_chunk_size) as reader:\n",
    "        while i < 3:\n",
    "            bytes = reader.read(uncompressed_chunk_size)\n",
    "            if not bytes:\n",
    "                break\n",
    "            # Process chunk of uncompressed data\n",
    "            print(f'Read {len(bytes)} bytes')\n",
    "            chunk = np.frombuffer(bytes, dtype=np.uint8)\n",
    "            print(f'chunk is {len(chunk)}')\n",
    "            print(bytes[0:10])\n",
    "            print(chunk[0:10])\n",
    "            i = i+1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03422ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "filebase = '/Users/lums/TileDB/feature-vector-prototype/external/data'\n",
    "array_base = 's3://tiledb-lums'\n",
    "\n",
    "files = ['siftsmall/siftsmall_base.fvecs',\n",
    "        'siftsmall/siftsmall_groundtruth.ivecs',\n",
    "        'siftsmall/siftsmall_learn.fvecs',\n",
    "        'siftsmall/siftsmall_query.fvecs',\n",
    "        'sift/sift_base.fvecs',\n",
    "        'sift/sift_groundtruth.ivecs',\n",
    "        'sift/sift_learn.fvecs',\n",
    "        'sift/sift_query.fvecs']\n",
    "arrays= ['siftsmall_base',\n",
    "        'siftsmall_groundtruth',\n",
    "        'siftsmall_learn',\n",
    "        'siftsmall_query',\n",
    "        'sift_base',\n",
    "        'sift_groundtruth',\n",
    "        'sift_learn',\n",
    "        'sift_query']\n",
    "\n",
    "for f, a in zip(files, arrays):\n",
    "    filename = os.path.join(filebase, f)\n",
    "    array_name = os.path.join(array_base, a)\n",
    "    print(f'{filename} -> {array_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb771b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2b892",
   "metadata": {},
   "source": [
    "### Some testing below here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558d9f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiledb\n",
    "\n",
    "filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "array_name = 's3://tiledb-lums/sift_base'\n",
    "# array_name = 'sift_base'\n",
    "\n",
    "A = tiledb.open(array_name)\n",
    "f = open(filename, 'rb')\n",
    "dim = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "\n",
    "chunk = np.fromfile(f, dtype=np.float32, count=dim * 1000000)\n",
    "B = chunk.reshape(1000000, dim)\n",
    "B = np.transpose(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e228e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'A is {type(A)} with shape {A.shape}')\n",
    "print(f'B is {type(A)} with shape {B.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d66c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk[0:10])\n",
    "print(A[0:10,0:5]['a'])\n",
    "print(B[0:10,0:5])\n",
    "\n",
    "A.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5dcffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abffd2d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ee917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "\n",
    "#bigann_query.bvecs.gz\n",
    "uncompressed_chunk_size = 1024 * 1024 # 1 MB chunk size\n",
    "i = 0\n",
    "with gzip.open('/Users/lums/TileDB/feature-vector-prototype/external/data/bigann_base.bvecs.gz', 'rb') as f:\n",
    "    with io.BufferedReader(f, buffer_size=uncompressed_chunk_size) as reader:\n",
    "        while i < 3:\n",
    "            bytes = reader.read(uncompressed_chunk_size)\n",
    "            if not bytes:\n",
    "                break\n",
    "            # Process chunk of uncompressed data\n",
    "            print(f'Read {len(bytes)} bytes')\n",
    "            chunk = np.frombuffer(bytes, dtype=np.uint8)\n",
    "            print(f'chunk is {len(chunk)}')\n",
    "            print(bytes[0:10])\n",
    "            print(chunk[0:10])\n",
    "            i = i+1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6ff80b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20474a39",
   "metadata": {},
   "source": [
    "### Cruft below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d599f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014216e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072a06f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd4c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the array to create.                                                                                                      \n",
    "array_name = \"writing_dense_multiple\"\n",
    "\n",
    "def create_array():\n",
    "    # The array will be 4x4 with dimensions \"rows\" and \"cols\", with domain [1,4].                                                   \n",
    "    dom = tiledb.Domain(\n",
    "        tiledb.Dim(name=\"rows\", domain=(0, 3), tile=2, dtype=np.int32),\n",
    "        tiledb.Dim(name=\"cols\", domain=(0, 4), tile=2, dtype=np.int32),\n",
    "    )\n",
    "\n",
    "    # The array will be dense with a single attribute \"a\" so each (i,j) cell can store an integer.                                  \n",
    "    schema = tiledb.ArraySchema(\n",
    "        domain=dom, sparse=False, attrs=[tiledb.Attr(name=\"a\", dtype=np.int32)]\n",
    "    )\n",
    "\n",
    "    # Create the (empty) array on disk.                                                                                             \n",
    "    tiledb.DenseArray.create(array_name, schema)\n",
    "\n",
    "\n",
    "def write_array():\n",
    "    # Open the array and write to it.                                                                                               \n",
    "    with tiledb.DenseArray(array_name, mode=\"w\") as A:\n",
    "        # First write                                                                                                               \n",
    "        data = np.array(([0, 22, 33], [2, 44, 77]))\n",
    "        A[0:2, 0:3] = data\n",
    "\n",
    "        # Second write                                                                                                              \n",
    "        data = np.array(([5, 6, 7, 8], [9, 10, 11, 12]))\n",
    "        A[1:3, 1:5] = data\n",
    "\n",
    "\n",
    "def read_array():\n",
    "    # Open the array and read from it.                                                                                              \n",
    "    with tiledb.DenseArray(array_name, mode=\"r\") as A:\n",
    "        # Slice the entire array                                                                                                    \n",
    "        data = A[:]\n",
    "        print(data[\"a\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "if tiledb.object_type(array_name) != \"array\":\n",
    "    create_array()\n",
    "    write_array()\n",
    "\n",
    "data = read_array()\n",
    "\n",
    "print(f'data is {type(data[\"a\"])} with shape {data[\"a\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9564a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Name of the array to create.                                                                                                      \n",
    "array_name = \"writing_dense_multiple\"\n",
    "\n",
    "\n",
    "def create_array():\n",
    "    # The array will be 4x4 with dimensions \"rows\" and \"cols\", with domain [1,4].                                                   \n",
    "    dom = tiledb.Domain(\n",
    "        tiledb.Dim(name=\"rows\", domain=(1, 4), tile=2, dtype=np.int32),\n",
    "        tiledb.Dim(name=\"cols\", domain=(1, 4), tile=2, dtype=np.int32),\n",
    "    )\n",
    "\n",
    "    # The array will be dense with a single attribute \"a\" so each (i,j) cell can store an integer.                                  \n",
    "    schema = tiledb.ArraySchema(\n",
    "        domain=dom, sparse=False, attrs=[tiledb.Attr(name=\"a\", dtype=np.int32)]\n",
    "    )\n",
    "\n",
    "    # Create the (empty) array on disk.                                                                                             \n",
    "    tiledb.DenseArray.create(array_name, schema)\n",
    "    \n",
    "            # First write                                                                                                               \n",
    "        data = np.array(([1, 2], [3, 4]))\n",
    "        A[1:3, 1:3] = data\n",
    "\n",
    "        # Second write                                                                                                              \n",
    "        data = np.array(([5, 6, 7, 8], [9, 10, 11, 12]))\n",
    "        A[2:4, 1:5] = data\n",
    "\n",
    "\n",
    "def write_array():\n",
    "    # Open the array and write to it.                                                                                               \n",
    "    with tiledb.DenseArray(array_name, mode=\"w\") as A:\n",
    "        # First write                                                                                                               \n",
    "        data = np.array(([1, 2], [3, 4]))\n",
    "        A[1:3, 1:3] = data\n",
    "\n",
    "        # Second write                                                                                                              \n",
    "        data = np.array(([5, 6, 7, 8], [9, 10, 11, 12]))\n",
    "        A[2:4, 1:5] = data\n",
    "\n",
    "\n",
    "def read_array():\n",
    "    # Open the array and read from it.                                                                                              \n",
    "    with tiledb.DenseArray(array_name, mode=\"r\") as A:\n",
    "        # Slice the entire array                                                                                                    \n",
    "        data = A[:]\n",
    "        print(data[\"a\"])\n",
    "\n",
    "\n",
    "if tiledb.object_type(array_name) != \"array\":\n",
    "    create_array()\n",
    "    write_array()\n",
    "\n",
    "read_array()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 'sift_base'\n",
    "array_name = 's3://tiledb-lums/sift_base'\n",
    "tile_size = 100000\n",
    "block_size = 10000\n",
    "create_fvecs_array(filename, array_name, tile_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca72aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fvecs_array(filename, array_name, tile_size, block_size):\n",
    "    with tiledb.from_numpy(array_name, np.zeros((0,))) as A:\n",
    "        dim = None\n",
    "        offset = 0\n",
    "        for block in fvecs_generator(filename, block_size):\n",
    "            if dim is None:\n",
    "                dim = block.shape[1]\n",
    "                A.schema.set_domain((0, None), (0, dim))\n",
    "                A.schema.set_tile((tile_size, dim))\n",
    "                A.schema.set_cell_order(tiledb.Layout.ROW_MAJOR)\n",
    "                A.schema.set_sparse(False)\n",
    "                A.schema.set_attrs(tiledb.Attr(\"features\", dtype=np.float32, var=False))\n",
    "            num_rows = block.shape[0]\n",
    "            A[offset:offset+num_rows] = block\n",
    "            offset += num_rows\n",
    "    A.schema.set_capacity(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a0bb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde04cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "block_size = 1000\n",
    "dimension = 128\n",
    "gen = fvecs_generator(filename, block_size, dimension)\n",
    "a = next(gen)\n",
    "print(f'a is {type(a)} with shape {a.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d89572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tiledb\n",
    "\n",
    "def fvecs_generator(filename, block_size):\n",
    "    f = open(filename, 'rb')\n",
    "    dim = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "    chunk_size = block_size * dim\n",
    "    while True:\n",
    "        chunk = np.fromfile(f, dtype=np.float32, count=chunk_size)\n",
    "        if chunk.size == 0:\n",
    "            break\n",
    "        yield chunk.reshape((-1, dim))\n",
    "    f.close()\n",
    "\n",
    "def create_fvecs_array(filename, array_name, tile_size, block_size):\n",
    "    with tiledb.from_numpy(array_name, np.zeros((0,))) as A:\n",
    "        dim = None\n",
    "        offset = 0\n",
    "        for block in fvecs_generator(filename, block_size):\n",
    "            if dim is None:\n",
    "                dim = block.shape[1]\n",
    "                A.schema.set_domain((0, None), (0, dim))\n",
    "                A.schema.set_tile((tile_size, dim))\n",
    "                A.schema.set_cell_order(tiledb.Layout.ROW_MAJOR)\n",
    "                A.schema.set_sparse(False)\n",
    "                A.schema.set_attrs(tiledb.Attr(\"features\", dtype=np.float32, var=False))\n",
    "            num_rows = block.shape[0]\n",
    "            A[offset:offset+num_rows] = block\n",
    "            offset += num_rows\n",
    "    A.schema.set_capacity(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'external/data/sift/sift_base.fvecs'\n",
    "array_name = 'sift_base'\n",
    "tile_size = 100000\n",
    "block_size = 10000\n",
    "create_fvecs_array(filename, array_name, tile_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9d41c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
