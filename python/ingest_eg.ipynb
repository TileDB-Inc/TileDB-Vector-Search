{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4d8ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tiledb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f42ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lums/TileDB/feature-vector-prototype/python\n"
     ]
    }
   ],
   "source": [
    "os.system('pwd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8791f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = tiledb.Ctx().config()\n",
    "\n",
    "# cfg[\"py.init_buffer_bytes\"] = 1024**2 * 50\n",
    "# cfg[\"vfs.s3.scheme\"] = \"https\" \n",
    "# cfg[\"vfs.s3.region\"] = \"us-west-2\"\n",
    "# cfg[\"vfs.s3.endpoint_override\"] = \"\"\n",
    "# cfg[\"vfs.s3.use_virtual_addressing\"] = \"true\"\n",
    "# cfg[\"vfs.s3.aws_access_key_id\"] = \"\";\n",
    "# cfg[\"vfs.s3.aws_secret_access_key\"] = \"\";\n",
    "\n",
    "tiledb.default_ctx({\"vfs.s3.region\": \"us-west-2\"});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f827405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_info(filename):\n",
    "    # Check if filename exists\n",
    "    if (not os.path.exists(filename)):\n",
    "        raise Exception(f'{filename} does not exist.')\n",
    "            \n",
    "    base, ext = os.path.splitext(filename)\n",
    "    if (ext == '.fvecs'):\n",
    "        element_type = np.float32\n",
    "        element_size = 4\n",
    "    elif (ext == '.ivecs'):\n",
    "        element_type = np.int32\n",
    "        element_size = 4\n",
    "    elif (ext == '.bvecs'):\n",
    "        element_type = np.uint8\n",
    "        element_size = 1\n",
    "    else:\n",
    "        raise Exception(f'Unkown extension {ext}')\n",
    "\n",
    "    file_size = os.path.getsize(filename)\n",
    "    print(f'The size of {filename} is {file_size} bytes.')\n",
    "\n",
    "    f = open(filename, 'rb')\n",
    "    dimension = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "    num_vectors = file_size // (4 + dimension * element_size)   # Four bytes for float or int\n",
    "\n",
    "    print(f'num_vectors is {num_vectors}, dimension is {dimension}')\n",
    "    f.close()\n",
    "    \n",
    "    return num_vectors, dimension, element_type"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 5,
>>>>>>> c2e45d54c276adf17888ff5143f598a88a1da4b0
>>>>>>> f953e628fc8c77e261ce0c598d7db75d78655b64
   "id": "dd8ab25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fvecs_generator(filename, num_vectors, dimension, block_size, element_type):\n",
    "    \n",
    "    f = open(filename, 'rb')\n",
    "    \n",
    "    num_vectors_read = 0\n",
    "    chunk_size = block_size * (dimension + 1)  # number of vectors by number of elements / vector\n",
    "    print(f'In generator chunk_size is {chunk_size}, block_size is {block_size}, dimension is {dimension}')\n",
    "    while True:\n",
    "        count = block_size\n",
    "        if ((num_vectors_read + block_size) > num_vectors):\n",
    "            count = (num_vectors_read + block_size) - num_vectors\n",
    "\n",
    "        chunk_size = count * (dimension + 1)\n",
    "        # chunk_size is number of elements\n",
    "        # if num_vectors // block_size == 0\n",
    "        # remaining vectors is num_vectors % block_size\n",
    "\n",
    "        chunk = np.fromfile(f, dtype=element_type, count=chunk_size)\n",
    "        if chunk.size == 0:\n",
    "            if (num_vectors_read != num_vectors):\n",
    "                raise Exception(f'{num_vectors_read} != {num_vectors}')\n",
    "            break\n",
    "        \n",
    "        # print(f'before reshape chunk is {type(chunk)} and size {chunk.shape}')\n",
    "        b = chunk.reshape(count, dimension+1)\n",
    "        b = np.transpose(b[:, 1:dimension+1])\n",
    "            \n",
    "        num_vectors_read = num_vectors_read + block_size\n",
    "\n",
    "        # print(f'after reshape chunk is {type(b)} and size {b.shape}')\n",
    "\n",
    "        yield b\n",
    "    print(f'total read {num_vectors_read}')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 6,
>>>>>>> c2e45d54c276adf17888ff5143f598a88a1da4b0
>>>>>>> f953e628fc8c77e261ce0c598d7db75d78655b64
   "id": "0fca2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array(array_name, num_vectors, dimension, tile_size, element_type):\n",
    "    \n",
    "    print(f'Creating array {array_name}: {dimension} by {num_vectors} with tile_size {tile_size}')\n",
    "    \n",
    "    # The array will be dimension by num_vectors                                                  \n",
    "    dom = tiledb.Domain(\n",
    "        tiledb.Dim(name=\"rows\", domain=(0, dimension-1), tile=dimension, dtype=np.int32),\n",
    "        tiledb.Dim(name=\"cols\", domain=(0, num_vectors-1), tile=tile_size, dtype=np.int32),\n",
    "    )\n",
    "\n",
    "    # The array will be dense with a single attribute \"a\" so each (i,j) cell can store an element_type.                                  \n",
    "    schema = tiledb.ArraySchema(\n",
    "        domain=dom, sparse=False, attrs=[tiledb.Attr(name=\"a\", dtype=element_type)], \n",
    "        cell_order='col-major', tile_order='col-major'\n",
    "    )\n",
    "\n",
    "    # Create the (empty) array on disk.   \n",
    "    if (tiledb.object_type(array_name) == \"array\"):\n",
    "        raise Exception(f\"Array {array_name} already exists!\")\n",
    "        print(f\"Array {array_name} already exists.  Deleting\")\n",
    "        tiledb.remove(array_name)\n",
    "    tiledb.DenseArray.create(array_name, schema)\n",
    "    \n",
    "    # Check size -- do not do this for anything large\n",
    "    # with tiledb.DenseArray(array_name, mode=\"r\") as A:\n",
    "    #    data = A[:]\n",
    "    #    print(f'After creation, array shape is {data[\"a\"].shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 7,
>>>>>>> c2e45d54c276adf17888ff5143f598a88a1da4b0
>>>>>>> f953e628fc8c77e261ce0c598d7db75d78655b64
   "id": "3e4dcdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_array(array_name, num_vectors, dimension, block_size, element_type):\n",
    "\n",
    "    print(f'Opening array {array_name} for writing with num_vectors {num_vectors} and dimension {dimension}')\n",
    "    \n",
    "    # Open the TileDB array for writing\n",
    "    A = tiledb.DenseArray(array_name, mode='w')\n",
    "\n",
    "    # Read data blocks from the file and write them to the TileDB array\n",
    "    generator = fvecs_generator(filename, num_vectors, dimension, block_size, element_type)\n",
    "\n",
    "    begin = 0\n",
    "    for block in generator:\n",
    "        # print(f'Read block number {begin} : block is {type(block)} with shape {block.shape}')\n",
    "        A[0:dimension, begin*block_size:(begin+1)*block_size] = block\n",
    "        begin = begin + 1\n",
    "        if begin*block_size >= num_vectors:\n",
    "            break\n",
    "\n",
    "    # Close the TileDB array\n",
    "    A.close()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
>>>>>>> f953e628fc8c77e261ce0c598d7db75d78655b64
   "execution_count": 11,
   "id": "205ea991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gist/gist_base.fvecs -> s3://tiledb-lums/sift/gist_base\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gist/gist_base.fvecs is 3844000000 bytes.\n",
      "num_vectors is 1000000, dimension is 960\n",
      "Creating array s3://tiledb-lums/sift/gist_base: 960 by 1000000 with tile_size 10000\n",
      "Opening array s3://tiledb-lums/sift/gist_base for writing with num_vectors 1000000 and dimension 960\n",
      "In generator chunk_size is 9610000, block_size is 10000, dimension is 960\n",
      "num_vectors is 1000000, dimension is 960, dtype is <class 'numpy.float32'>, tile_size is 10000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gist/gist_groundtruth.ivecs -> s3://tiledb-lums/sift/gist_groundtruth\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gist/gist_groundtruth.ivecs is 404000 bytes.\n",
      "num_vectors is 1000, dimension is 100\n",
      "Creating array s3://tiledb-lums/sift/gist_groundtruth: 100 by 1000 with tile_size 1000\n",
      "Opening array s3://tiledb-lums/sift/gist_groundtruth for writing with num_vectors 1000 and dimension 100\n",
      "In generator chunk_size is 101000, block_size is 1000, dimension is 100\n",
      "num_vectors is 1000, dimension is 100, dtype is <class 'numpy.int32'>, tile_size is 1000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_learn.fvecs -> s3://tiledb-lums/sift/gist_learn\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_learn.fvecs is 51600000 bytes.\n",
      "num_vectors is 100000, dimension is 128\n",
      "Creating array s3://tiledb-lums/sift/gist_learn: 128 by 100000 with tile_size 10000\n",
      "Opening array s3://tiledb-lums/sift/gist_learn for writing with num_vectors 100000 and dimension 128\n",
      "In generator chunk_size is 1290000, block_size is 10000, dimension is 128\n",
      "num_vectors is 100000, dimension is 128, dtype is <class 'numpy.float32'>, tile_size is 10000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gist/gist_query.fvecs -> s3://tiledb-lums/sift/gist_query\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gist/gist_query.fvecs is 3844000 bytes.\n",
      "num_vectors is 1000, dimension is 960\n",
      "Creating array s3://tiledb-lums/sift/gist_query: 960 by 1000 with tile_size 1000\n",
      "Opening array s3://tiledb-lums/sift/gist_query for writing with num_vectors 1000 and dimension 960\n",
      "In generator chunk_size is 961000, block_size is 1000, dimension is 960\n",
      "num_vectors is 1000, dimension is 960, dtype is <class 'numpy.float32'>, tile_size is 1000\n"
     ]
    }
   ],
   "source": [
    "const_block_size = 10000\n",
    "# const_dim = 128\n",
    "tile_size = const_block_size\n",
    "# tile_size = const_dim\n",
    "\n",
    "filebase = '/Users/lums/TileDB/feature-vector-prototype/external/data'\n",
    "array_base = 's3://tiledb-lums/sift'\n",
    "\n",
    "files = ['gist/gist_base.fvecs',\n",
    "         'gist/gist_groundtruth.ivecs',\n",
    "         'sift/sift_learn.fvecs',\n",
    "         'gist/gist_query.fvecs']\n",
    "\n",
    "arrays= ['gist_base',\n",
    "         'gist_groundtruth',\n",
    "         'gist_learn',\n",
    "         'gist_query']\n",
    "\n",
    "for f, a in zip(files, arrays):\n",
    "    filename = os.path.join(filebase, f)\n",
    "    array_name = os.path.join(array_base, a)\n",
    "    print(f'{filename} -> {array_name}')\n",
    "    \n",
    "    base, ext = os.path.splitext(filename)\n",
    "\n",
    "    num_vectors, dimension, dtype = get_data_info(filename)\n",
    "    tile_size = min(num_vectors, const_block_size)\n",
    "    block_size = min(num_vectors, const_block_size)\n",
    "    create_array(array_name, num_vectors, dimension, tile_size, dtype)\n",
    "    write_array(array_name, num_vectors, dimension, block_size, dtype)\n",
    "    \n",
    "    print(f'num_vectors is {num_vectors}, dimension is {dimension}, dtype is {dtype}, tile_size is {tile_size}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
>>>>>>> c2e45d54c276adf17888ff5143f598a88a1da4b0
>>>>>>> f953e628fc8c77e261ce0c598d7db75d78655b64
   "execution_count": 19,
   "id": "c14b786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_base.fvecs -> /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_base\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_base.fvecs is 5160000 bytes.\n",
      "num_vectors is 10000, dimension is 128\n",
      "Creating array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_base: 128 by 10000 with tile_size 10000\n",
      "Opening array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_base for writing with num_vectors 10000 and dimension 128\n",
      "In generator chunk_size is 1290000, block_size is 10000, dimension is 128\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_groundtruth.ivecs -> /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_groundtruth\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_groundtruth.ivecs is 40400 bytes.\n",
      "num_vectors is 100, dimension is 100\n",
      "Creating array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_groundtruth: 100 by 100 with tile_size 100\n",
      "Opening array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_groundtruth for writing with num_vectors 100 and dimension 100\n",
      "In generator chunk_size is 10100, block_size is 100, dimension is 100\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_learn.fvecs -> /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_learn\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_learn.fvecs is 12900000 bytes.\n",
      "num_vectors is 25000, dimension is 128\n",
      "Creating array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_learn: 128 by 25000 with tile_size 10000\n",
      "Opening array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_learn for writing with num_vectors 25000 and dimension 128\n",
      "In generator chunk_size is 1290000, block_size is 10000, dimension is 128\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_query.fvecs -> /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_query\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_query.fvecs is 51600 bytes.\n",
      "num_vectors is 100, dimension is 128\n",
      "Creating array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_query: 128 by 100 with tile_size 100\n",
      "Opening array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/siftsmall_query for writing with num_vectors 100 and dimension 128\n",
      "In generator chunk_size is 12900, block_size is 100, dimension is 128\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs -> /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_base\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs is 516000000 bytes.\n",
      "num_vectors is 1000000, dimension is 128\n",
      "Creating array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_base: 128 by 1000000 with tile_size 10000\n",
      "Opening array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_base for writing with num_vectors 1000000 and dimension 128\n",
      "In generator chunk_size is 1290000, block_size is 10000, dimension is 128\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_groundtruth.ivecs -> /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_groundtruth\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_groundtruth.ivecs is 4040000 bytes.\n",
      "num_vectors is 10000, dimension is 100\n",
      "Creating array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_groundtruth: 100 by 10000 with tile_size 10000\n",
      "Opening array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_groundtruth for writing with num_vectors 10000 and dimension 100\n",
      "In generator chunk_size is 1010000, block_size is 10000, dimension is 100\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_learn.fvecs -> /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_learn\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_learn.fvecs is 51600000 bytes.\n",
      "num_vectors is 100000, dimension is 128\n",
      "Creating array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_learn: 128 by 100000 with tile_size 10000\n",
      "Opening array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_learn for writing with num_vectors 100000 and dimension 128\n",
      "In generator chunk_size is 1290000, block_size is 10000, dimension is 128\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_query.fvecs -> /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_query\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_query.fvecs is 5160000 bytes.\n",
      "num_vectors is 10000, dimension is 128\n",
      "Creating array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_query: 128 by 10000 with tile_size 10000\n",
      "Opening array /Users/lums/TileDB/feature-vector-prototype/external/data/arrays/sift_query for writing with num_vectors 10000 and dimension 128\n",
      "In generator chunk_size is 1290000, block_size is 10000, dimension is 128\n"
     ]
    }
   ],
   "source": [
    "const_block_size = 10000\n",
    "# const_dim = 128\n",
    "tile_size = const_block_size\n",
    "# tile_size = const_dim\n",
    "\n",
    "\n",
    "# filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 's3://tiledb-lums/sift_base'\n",
    "\n",
    "\n",
    "filebase = '/Users/lums/TileDB/feature-vector-prototype/external/data'\n",
    "#array_base = 's3://tiledb-lums'\n",
    "array_base = '/Users/lums/TileDB/feature-vector-prototype/external/data/arrays'\n",
    "\n",
    "files = ['siftsmall/siftsmall_base.fvecs',\n",
    "        'siftsmall/siftsmall_groundtruth.ivecs',\n",
    "        'siftsmall/siftsmall_learn.fvecs',\n",
    "        'siftsmall/siftsmall_query.fvecs',\n",
    "        'sift/sift_base.fvecs',\n",
    "        'sift/sift_groundtruth.ivecs',\n",
    "        'sift/sift_learn.fvecs',\n",
    "        'sift/sift_query.fvecs']\n",
    "\n",
    "arrays= ['siftsmall_base',\n",
    "        'siftsmall_groundtruth',\n",
    "        'siftsmall_learn',\n",
    "        'siftsmall_query',\n",
    "        'sift_base',\n",
    "        'sift_groundtruth',\n",
    "        'sift_learn',\n",
    "        'sift_query']\n",
    "\n",
    "for f, a in zip(files, arrays):\n",
    "    filename = os.path.join(filebase, f)\n",
    "    array_name = os.path.join(array_base, a)\n",
    "    print(f'{filename} -> {array_name}')\n",
    "    \n",
    "    base, ext = os.path.splitext(filename)\n",
    "\n",
    "    num_vectors, dimension, dtype = get_data_info(filename)\n",
    "    tile_size = min(num_vectors, const_block_size)\n",
    "    block_size = min(num_vectors, const_block_size)\n",
    "    create_array(array_name, num_vectors, dimension, tile_size, dtype)\n",
    "    write_array(array_name, num_vectors, dimension, block_size, dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ea1081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_base.fvecs -> s3://tiledb-lums/sift/siftsmall_base\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_groundtruth.ivecs -> s3://tiledb-lums/sift/siftsmall_groundtruth\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_groundtruth.ivecs is 40400 bytes.\n",
      "num_vectors is 100, dimension is 100\n",
      "Creating array s3://tiledb-lums/sift/siftsmall_groundtruth: 100 by 100 with tile_size 100\n",
      "Array s3://tiledb-lums/sift/siftsmall_groundtruth already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/siftsmall_groundtruth for writing with num_vectors 100 and dimension 100\n",
      "In generator chunk_size is 10100, block_size is 100, dimension is 100\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_learn.fvecs -> s3://tiledb-lums/sift/siftsmall_learn\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/siftsmall/siftsmall_query.fvecs -> s3://tiledb-lums/sift/siftsmall_query\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs -> s3://tiledb-lums/sift/sift_base\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_groundtruth.ivecs -> s3://tiledb-lums/sift/sift_groundtruth\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_groundtruth.ivecs is 4040000 bytes.\n",
      "num_vectors is 10000, dimension is 100\n",
      "Creating array s3://tiledb-lums/sift/sift_groundtruth: 100 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/sift_groundtruth already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/sift_groundtruth for writing with num_vectors 10000 and dimension 100\n",
      "In generator chunk_size is 1010000, block_size is 10000, dimension is 100\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_learn.fvecs -> s3://tiledb-lums/sift/sift_learn\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_query.fvecs -> s3://tiledb-lums/sift/sift_query\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/dis_1000M.fvecs -> s3://tiledb-lums/sift/gnd/dis_1000M\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/dis_100M.fvecs -> s3://tiledb-lums/sift/gnd/dis_100M\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/dis_10M.fvecs -> s3://tiledb-lums/sift/gnd/dis_10M\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/dis_1M.fvecs -> s3://tiledb-lums/sift/gnd/dis_1M\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/dis_200M.fvecs -> s3://tiledb-lums/sift/gnd/dis_200M\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/dis_20M.fvecs -> s3://tiledb-lums/sift/gnd/dis_20M\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/dis_2M.fvecs -> s3://tiledb-lums/sift/gnd/dis_2M\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/dis_500M.fvecs -> s3://tiledb-lums/sift/gnd/dis_500M\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/dis_50M.fvecs -> s3://tiledb-lums/sift/gnd/dis_50M\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/dis_5M.fvecs -> s3://tiledb-lums/sift/gnd/dis_5M\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_1000M.ivecs -> s3://tiledb-lums/sift/gnd/idx_1000M\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_1000M.ivecs is 40040000 bytes.\n",
      "num_vectors is 10000, dimension is 1000\n",
      "Creating array s3://tiledb-lums/sift/gnd/idx_1000M: 1000 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/gnd/idx_1000M already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/gnd/idx_1000M for writing with num_vectors 10000 and dimension 1000\n",
      "In generator chunk_size is 10010000, block_size is 10000, dimension is 1000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_100M.ivecs -> s3://tiledb-lums/sift/gnd/idx_100M\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_100M.ivecs is 40040000 bytes.\n",
      "num_vectors is 10000, dimension is 1000\n",
      "Creating array s3://tiledb-lums/sift/gnd/idx_100M: 1000 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/gnd/idx_100M already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/gnd/idx_100M for writing with num_vectors 10000 and dimension 1000\n",
      "In generator chunk_size is 10010000, block_size is 10000, dimension is 1000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_10M.ivecs -> s3://tiledb-lums/sift/gnd/idx_10M\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_10M.ivecs is 40040000 bytes.\n",
      "num_vectors is 10000, dimension is 1000\n",
      "Creating array s3://tiledb-lums/sift/gnd/idx_10M: 1000 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/gnd/idx_10M already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/gnd/idx_10M for writing with num_vectors 10000 and dimension 1000\n",
      "In generator chunk_size is 10010000, block_size is 10000, dimension is 1000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_1M.ivecs -> s3://tiledb-lums/sift/gnd/idx_1M\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_1M.ivecs is 40040000 bytes.\n",
      "num_vectors is 10000, dimension is 1000\n",
      "Creating array s3://tiledb-lums/sift/gnd/idx_1M: 1000 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/gnd/idx_1M already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/gnd/idx_1M for writing with num_vectors 10000 and dimension 1000\n",
      "In generator chunk_size is 10010000, block_size is 10000, dimension is 1000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_200M.ivecs -> s3://tiledb-lums/sift/gnd/idx_200M\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_200M.ivecs is 40040000 bytes.\n",
      "num_vectors is 10000, dimension is 1000\n",
      "Creating array s3://tiledb-lums/sift/gnd/idx_200M: 1000 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/gnd/idx_200M already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/gnd/idx_200M for writing with num_vectors 10000 and dimension 1000\n",
      "In generator chunk_size is 10010000, block_size is 10000, dimension is 1000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_20M.ivecs -> s3://tiledb-lums/sift/gnd/idx_20M\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_20M.ivecs is 40040000 bytes.\n",
      "num_vectors is 10000, dimension is 1000\n",
      "Creating array s3://tiledb-lums/sift/gnd/idx_20M: 1000 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/gnd/idx_20M already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/gnd/idx_20M for writing with num_vectors 10000 and dimension 1000\n",
      "In generator chunk_size is 10010000, block_size is 10000, dimension is 1000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_2M.ivecs -> s3://tiledb-lums/sift/gnd/idx_2M\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_2M.ivecs is 40040000 bytes.\n",
      "num_vectors is 10000, dimension is 1000\n",
      "Creating array s3://tiledb-lums/sift/gnd/idx_2M: 1000 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/gnd/idx_2M already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/gnd/idx_2M for writing with num_vectors 10000 and dimension 1000\n",
      "In generator chunk_size is 10010000, block_size is 10000, dimension is 1000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_500M.ivecs -> s3://tiledb-lums/sift/gnd/idx_500M\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_500M.ivecs is 40040000 bytes.\n",
      "num_vectors is 10000, dimension is 1000\n",
      "Creating array s3://tiledb-lums/sift/gnd/idx_500M: 1000 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/gnd/idx_500M already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/gnd/idx_500M for writing with num_vectors 10000 and dimension 1000\n",
      "In generator chunk_size is 10010000, block_size is 10000, dimension is 1000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_50M.ivecs -> s3://tiledb-lums/sift/gnd/idx_50M\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_50M.ivecs is 40040000 bytes.\n",
      "num_vectors is 10000, dimension is 1000\n",
      "Creating array s3://tiledb-lums/sift/gnd/idx_50M: 1000 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/gnd/idx_50M already exists.  Deleting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening array s3://tiledb-lums/sift/gnd/idx_50M for writing with num_vectors 10000 and dimension 1000\n",
      "In generator chunk_size is 10010000, block_size is 10000, dimension is 1000\n",
      "/Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_5M.ivecs -> s3://tiledb-lums/sift/gnd/idx_5M\n",
      "The size of /Users/lums/TileDB/feature-vector-prototype/external/data/gnd/idx_5M.ivecs is 40040000 bytes.\n",
      "num_vectors is 10000, dimension is 1000\n",
      "Creating array s3://tiledb-lums/sift/gnd/idx_5M: 1000 by 10000 with tile_size 10000\n",
      "Array s3://tiledb-lums/sift/gnd/idx_5M already exists.  Deleting\n",
      "Opening array s3://tiledb-lums/sift/gnd/idx_5M for writing with num_vectors 10000 and dimension 1000\n",
      "In generator chunk_size is 10010000, block_size is 10000, dimension is 1000\n"
     ]
    }
   ],
   "source": [
    "# filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 'sift_base'\n",
    "# array_name = 's3://tiledb-lums/sift_base'\n",
    "# array_name = 'tiledb://lums/sift_base'\n",
    "# array_name = 'https://tiledb-lums.s3.amazonaws.com/sift_base'\n",
    "# array_name = 'https://tiledb-lums.s3-us-west-2.amazonaws.com/sift_base'\n",
    "\n",
    "const_block_size = 10000\n",
    "# const_dim = 128\n",
    "tile_size = const_block_size\n",
    "# tile_size = const_dim\n",
    "\n",
    "\n",
    "# filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 's3://tiledb-lums/sift_base'\n",
    "\n",
    "\n",
    "filebase = '/Users/lums/TileDB/feature-vector-prototype/external/data'\n",
    "array_base = 's3://tiledb-lums'\n",
    "\n",
    "files = ['siftsmall/siftsmall_base.fvecs',\n",
    "        'siftsmall/siftsmall_groundtruth.ivecs',\n",
    "        'siftsmall/siftsmall_learn.fvecs',\n",
    "        'siftsmall/siftsmall_query.fvecs',\n",
    "        'sift/sift_base.fvecs',\n",
    "        'sift/sift_groundtruth.ivecs',\n",
    "        'sift/sift_learn.fvecs',\n",
    "        'sift/sift_query.fvecs',\n",
    "        'gnd/dis_1000M.fvecs',\n",
    "        'gnd/dis_100M.fvecs',\n",
    "        'gnd/dis_10M.fvecs',\n",
    "        'gnd/dis_1M.fvecs',\n",
    "        'gnd/dis_200M.fvecs',\n",
    "        'gnd/dis_20M.fvecs',\n",
    "        'gnd/dis_2M.fvecs',\n",
    "        'gnd/dis_500M.fvecs',\n",
    "        'gnd/dis_50M.fvecs',\n",
    "        'gnd/dis_5M.fvecs',\n",
    "        'gnd/idx_1000M.ivecs',\n",
    "        'gnd/idx_100M.ivecs',\n",
    "        'gnd/idx_10M.ivecs',\n",
    "        'gnd/idx_1M.ivecs',\n",
    "        'gnd/idx_200M.ivecs',\n",
    "        'gnd/idx_20M.ivecs',\n",
    "        'gnd/idx_2M.ivecs',\n",
    "        'gnd/idx_500M.ivecs',\n",
    "        'gnd/idx_50M.ivecs',\n",
    "        'gnd/idx_5M.ivecs']\n",
    "\n",
    "arrays= ['siftsmall_base',\n",
    "        'siftsmall_groundtruth',\n",
    "        'siftsmall_learn',\n",
    "        'siftsmall_query',\n",
    "        'sift_base',\n",
    "        'sift_groundtruth',\n",
    "        'sift_learn',\n",
    "        'sift_query',\n",
    "        'gnd/dis_1000M',\n",
    "        'gnd/dis_100M',\n",
    "        'gnd/dis_10M',\n",
    "        'gnd/dis_1M',\n",
    "        'gnd/dis_200M',\n",
    "        'gnd/dis_20M',\n",
    "        'gnd/dis_2M',\n",
    "        'gnd/dis_500M',\n",
    "        'gnd/dis_50M',\n",
    "        'gnd/dis_5M',\n",
    "        'gnd/idx_1000M',\n",
    "        'gnd/idx_100M',\n",
    "        'gnd/idx_10M',\n",
    "        'gnd/idx_1M',\n",
    "        'gnd/idx_200M',\n",
    "        'gnd/idx_20M',\n",
    "        'gnd/idx_2M',\n",
    "        'gnd/idx_500M',\n",
    "        'gnd/idx_50M',\n",
    "        'gnd/idx_5M']\n",
    "\n",
    "filebase = '/Users/lums/TileDB/feature-vector-prototype/external/data'\n",
    "array_base = 's3://tiledb-lums/sift'\n",
    "\n",
    "for f, a in zip(files, arrays):\n",
    "    filename = os.path.join(filebase, f)\n",
    "    array_name = os.path.join(array_base, a)\n",
    "    print(f'{filename} -> {array_name}')\n",
    "    \n",
    "    base, ext = os.path.splitext(filename)\n",
    "\n",
    "    num_vectors, dimension, dtype = get_data_info(filename)\n",
    "    tile_size = min(num_vectors, const_block_size)\n",
    "    block_size = min(num_vectors, const_block_size)\n",
    "    create_array(array_name, num_vectors, dimension, tile_size, dtype)\n",
    "    write_array(array_name, num_vectors, dimension, block_size, dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058eb394",
   "metadata": {},
   "source": [
    "### For bigann compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_bvecs_gz(filename):\n",
    "    # Check if filename exists\n",
    "    if (not os.path.exists(filename)):\n",
    "        raise Exception(f'{filename} does not exist.')\n",
    "    base, ext = os.path.splitext(filename)\n",
    "    if (ext != '.gz'):\n",
    "        raise Exception(f'{filename} is not a gz file')\n",
    "    \n",
    "    base, ext = os.path.splitext(base)\n",
    "    if (ext != '.bvecs'):\n",
    "        raise Exception(f'{filename} is not a bvecs file')\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b72580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gz_data_info(filename):\n",
    "    \n",
    "    # Check if filename exists\n",
    "    if (not is_valid_bvecs_gz(filename)):\n",
    "        raise Exception(f'Invalid file {filename}')\n",
    "\n",
    "    file_size = os.path.getsize(filename)\n",
    "    print(f'The size of the compressed {filename} is {file_size} bytes.')\n",
    "\n",
    "    # bigann_query.bvecs.gz\n",
    "    uncompressed_chunk_size = 4   # one int32\n",
    "\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        with io.BufferedReader(f, buffer_size=uncompressed_chunk_size) as reader:\n",
    "            bytes = reader.read(uncompressed_chunk_size)\n",
    "            if not bytes:\n",
    "                raise Exception('Could not get dimension size')\n",
    "            # Process chunk of uncompressed data\n",
    "            print(f'Read {len(bytes)} bytes')\n",
    "\n",
    "            dimension = np.frombuffer(bytes, dtype=np.int32, count=1)[0]\n",
    "            print(f'dimension is {dimension}')\n",
    "       \n",
    "    f.close()\n",
    "    \n",
    "    return None, dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bvecs_gz_generator(filename, num_vectors, dimension, block_size):\n",
    "    \n",
    "    # Check if filename exists\n",
    "    if (not is_valid_bvecs_gz(filename)):\n",
    "        raise Exception(f'Invalid file {filename}')\n",
    "    \n",
    "    element_type = np.uint8\n",
    "    \n",
    "    uncompressed_chunk_size = block_size * (dimension + 1)\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        with io.BufferedReader(f, buffer_size=uncompressed_chunk_size) as reader:\n",
    "                \n",
    "            num_vectors_read = 0\n",
    "            \n",
    "            print(f'In generator uncompressed_chunk_size is {uncompressed_chunk_size}, block_size is {block_size}, dimension is {dimension}')\n",
    "            print(f'        num_vectors is {num_vectors}')\n",
    "            while True:\n",
    "                count = block_size\n",
    "                if ((num_vectors_read + block_size) > num_vectors):\n",
    "                    count = (num_vectors_read + block_size) - num_vectors\n",
    "\n",
    "                uncompressed_chunk_size = count * (dimension + 1)\n",
    "                bytes = reader.read(uncompressed_chunk_size * 4)\n",
    "                \n",
    "                print(f'In generator uncompressed_chunk_size is {uncompressed_chunk_size}, block_size is {block_size}, dimension is {dimension}')\n",
    "                print(f'    count is {count}, lenbytes is {len(bytes)}')\n",
    "                if (len(bytes) == 0):\n",
    "                    print(f'end of file len_bytes is {len(bytes)}')\n",
    "                    break\n",
    "                chunk = np.frombuffer(bytes, dtype=element_type, count=uncompressed_chunk_size)\n",
    "                \n",
    "                if chunk.size == 0:\n",
    "                    print(f'end of file')\n",
    "                    if (num_vectors_read != num_vectors):\n",
    "                        raise Exception(f'{num_vectors_read} != {num_vectors}')\n",
    "                    break\n",
    "                if (num_vectors_read == num_vectors):\n",
    "                    print(f'read to {num_vectors_read} == {num_vectors}')\n",
    "                    break\n",
    "        \n",
    "                # print(f'before reshape chunk is {type(chunk)} and size {chunk.shape}')\n",
    "                b = chunk.reshape(count, dimension+1)\n",
    "                b = np.transpose(b[:, 1:dimension+1])\n",
    "                    \n",
    "                num_vectors_read = num_vectors_read + block_size\n",
    "                \n",
    "                print(f'num_vectors_read is {num_vectors_read} of {num_vectors}')\n",
    "\n",
    "                # print(f'after reshape chunk is {type(b)} and size {b.shape}')\n",
    "\n",
    "                yield b\n",
    "       \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_gz_array(array_name, num_vectors, dimension, block_size):\n",
    "\n",
    "    print(f'Opening array {array_name} for writing with num_vectors {num_vectors} and dimension {dimension}')\n",
    "    \n",
    "    # Open the TileDB array for writing\n",
    "    A = tiledb.DenseArray(array_name, mode='w')\n",
    "\n",
    "    # Read data blocks from the file and write them to the TileDB array\n",
    "    generator = bvecs_gz_generator(filename, num_vectors, dimension, block_size)\n",
    "\n",
    "    begin = 0\n",
    "    for block in generator:\n",
    "        \n",
    "        print(f'Read block number {begin} : block is {type(block)} with shape {block.shape}')\n",
    "        \n",
    "        A[0:dimension, begin*block_size:(begin+1)*block_size] = block\n",
    "        begin = begin + 1\n",
    "        if begin*block_size >= num_vectors:\n",
    "            break\n",
    "\n",
    "    # Close the TileDB array\n",
    "    A.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5794c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 'sift_base'\n",
    "# array_name = 's3://tiledb-lums/sift_base'\n",
    "# array_name = 'tiledb://lums/sift_base'\n",
    "# array_name = 'https://tiledb-lums.s3.amazonaws.com/sift_base'\n",
    "# array_name = 'https://tiledb-lums.s3-us-west-2.amazonaws.com/sift_base'\n",
    "\n",
    "const_block_size = 1000000\n",
    "# const_dim = 128\n",
    "tile_size = const_block_size\n",
    "# tile_size = const_dim\n",
    "\n",
    "\n",
    "# filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 's3://tiledb-lums/sift_base'\n",
    "\n",
    "\n",
    "filebase = '/Users/lums/TileDB/feature-vector-prototype/external/data'\n",
    "array_base = 's3://tiledb-lums/sift'\n",
    "\n",
    "files = ['bigann_query.bvecs.gz',\n",
    "         'bigann_learn.bvecs.gz',\n",
    "         'bigann_base.bvecs.gz']\n",
    "\n",
    "arrays = ['bigann_query',\n",
    "          'bigann_learn',\n",
    "          'bigann_base']\n",
    "\n",
    "nums = [ 10000,\n",
    "         100000000,\n",
    "         1000000000]\n",
    "\n",
    "for f, a, num_vectors in zip(files, arrays, nums):\n",
    "    filename = os.path.join(filebase, f)\n",
    "    array_name = os.path.join(array_base, a)\n",
    "    print(f'{filename} -> {array_name}')\n",
    "\n",
    "    _, dimension = get_gz_data_info(filename)\n",
    "    tile_size = min(num_vectors, const_block_size)\n",
    "    block_size = min(num_vectors, const_block_size)\n",
    "    create_array(array_name, num_vectors, dimension, tile_size)\n",
    "    write_gz_array(array_name, num_vectors, dimension, block_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1962f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83084913",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/bigann_base.bvecs.gz'\n",
    "a, b = get_gz_data(filename)\n",
    "print(f'dimension b is {b}')\n",
    "g = fvecs_gz_generator(filename, 10, b, 5)\n",
    "print(f'g is {type(g)}')\n",
    "j = 0\n",
    "for i in g:\n",
    "    print(f'i is {type(i)} with shape {i.shape}')\n",
    "    if (j == 5):\n",
    "        break\n",
    "    j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb188ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f0642e0",
   "metadata": {},
   "source": [
    "### Some prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed267345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#bigann_query.bvecs.gz\n",
    "uncompressed_chunk_size = 1024 * 1024 # 1 MB chunk size\n",
    "i = 0\n",
    "with gzip.open('/Users/lums/TileDB/feature-vector-prototype/external/data/bigann_base.bvecs.gz', 'rb') as f:\n",
    "    with io.BufferedReader(f, buffer_size=uncompressed_chunk_size) as reader:\n",
    "        while i < 3:\n",
    "            bytes = reader.read(uncompressed_chunk_size)\n",
    "            if not bytes:\n",
    "                break\n",
    "            # Process chunk of uncompressed data\n",
    "            print(f'Read {len(bytes)} bytes')\n",
    "            chunk = np.frombuffer(bytes, dtype=np.uint8)\n",
    "            print(f'chunk is {len(chunk)}')\n",
    "            print(bytes[0:10])\n",
    "            print(chunk[0:10])\n",
    "            i = i+1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52df878",
   "metadata": {},
   "outputs": [],
   "source": [
    "filebase = '/Users/lums/TileDB/feature-vector-prototype/external/data'\n",
    "array_base = 's3://tiledb-lums'\n",
    "\n",
    "files = ['siftsmall/siftsmall_base.fvecs',\n",
    "        'siftsmall/siftsmall_groundtruth.ivecs',\n",
    "        'siftsmall/siftsmall_learn.fvecs',\n",
    "        'siftsmall/siftsmall_query.fvecs',\n",
    "        'sift/sift_base.fvecs',\n",
    "        'sift/sift_groundtruth.ivecs',\n",
    "        'sift/sift_learn.fvecs',\n",
    "        'sift/sift_query.fvecs']\n",
    "arrays= ['siftsmall_base',\n",
    "        'siftsmall_groundtruth',\n",
    "        'siftsmall_learn',\n",
    "        'siftsmall_query',\n",
    "        'sift_base',\n",
    "        'sift_groundtruth',\n",
    "        'sift_learn',\n",
    "        'sift_query']\n",
    "\n",
    "for f, a in zip(files, arrays):\n",
    "    filename = os.path.join(filebase, f)\n",
    "    array_name = os.path.join(array_base, a)\n",
    "    print(f'{filename} -> {array_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5850fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10eb2e",
   "metadata": {},
   "source": [
    "### Some testing below here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b8d14d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiledb\n",
    "\n",
    "filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "array_name = 's3://tiledb-lums/sift_base'\n",
    "# array_name = 'sift_base'\n",
    "\n",
    "A = tiledb.open(array_name)\n",
    "f = open(filename, 'rb')\n",
    "dim = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "\n",
    "chunk = np.fromfile(f, dtype=np.float32, count=dim * 1000000)\n",
    "B = chunk.reshape(1000000, dim)\n",
    "B = np.transpose(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d33123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'A is {type(A)} with shape {A.shape}')\n",
    "print(f'B is {type(A)} with shape {B.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dce642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk[0:10])\n",
    "print(A[0:10,0:5]['a'])\n",
    "print(B[0:10,0:5])\n",
    "\n",
    "A.close()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53255008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0547e89",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import io\n",
    "\n",
    "#bigann_query.bvecs.gz\n",
    "uncompressed_chunk_size = 1024 * 1024 # 1 MB chunk size\n",
    "i = 0\n",
    "with gzip.open('/Users/lums/TileDB/feature-vector-prototype/external/data/bigann_base.bvecs.gz', 'rb') as f:\n",
    "    with io.BufferedReader(f, buffer_size=uncompressed_chunk_size) as reader:\n",
    "        while i < 3:\n",
    "            bytes = reader.read(uncompressed_chunk_size)\n",
    "            if not bytes:\n",
    "                break\n",
    "            # Process chunk of uncompressed data\n",
    "            print(f'Read {len(bytes)} bytes')\n",
    "            chunk = np.frombuffer(bytes, dtype=np.uint8)\n",
    "            print(f'chunk is {len(chunk)}')\n",
    "            print(bytes[0:10])\n",
    "            print(chunk[0:10])\n",
    "            i = i+1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812bf14f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f707963d",
   "metadata": {},
   "source": [
    "### Cruft below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960f94ec",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4da98e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210bd78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac51f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the array to create.                                                                                                      \n",
    "array_name = \"writing_dense_multiple\"\n",
    "\n",
    "def create_array():\n",
    "    # The array will be 4x4 with dimensions \"rows\" and \"cols\", with domain [1,4].                                                   \n",
    "    dom = tiledb.Domain(\n",
    "        tiledb.Dim(name=\"rows\", domain=(0, 3), tile=2, dtype=np.int32),\n",
    "        tiledb.Dim(name=\"cols\", domain=(0, 4), tile=2, dtype=np.int32),\n",
    "    )\n",
    "\n",
    "    # The array will be dense with a single attribute \"a\" so each (i,j) cell can store an integer.                                  \n",
    "    schema = tiledb.ArraySchema(\n",
    "        domain=dom, sparse=False, attrs=[tiledb.Attr(name=\"a\", dtype=np.int32)]\n",
    "    )\n",
    "\n",
    "    # Create the (empty) array on disk.                                                                                             \n",
    "    tiledb.DenseArray.create(array_name, schema)\n",
    "\n",
    "\n",
    "def write_array():\n",
    "    # Open the array and write to it.                                                                                               \n",
    "    with tiledb.DenseArray(array_name, mode=\"w\") as A:\n",
    "        # First write                                                                                                               \n",
    "        data = np.array(([0, 22, 33], [2, 44, 77]))\n",
    "        A[0:2, 0:3] = data\n",
    "\n",
    "        # Second write                                                                                                              \n",
    "        data = np.array(([5, 6, 7, 8], [9, 10, 11, 12]))\n",
    "        A[1:3, 1:5] = data\n",
    "\n",
    "\n",
    "def read_array():\n",
    "    # Open the array and read from it.                                                                                              \n",
    "    with tiledb.DenseArray(array_name, mode=\"r\") as A:\n",
    "        # Slice the entire array                                                                                                    \n",
    "        data = A[:]\n",
    "        print(data[\"a\"])\n",
    "    return data\n",
    "\n",
    "\n",
    "if tiledb.object_type(array_name) != \"array\":\n",
    "    create_array()\n",
    "    write_array()\n",
    "\n",
    "data = read_array()\n",
    "\n",
    "print(f'data is {type(data[\"a\"])} with shape {data[\"a\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8314eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Name of the array to create.                                                                                                      \n",
    "array_name = \"writing_dense_multiple\"\n",
    "\n",
    "\n",
    "def create_array():\n",
    "    # The array will be 4x4 with dimensions \"rows\" and \"cols\", with domain [1,4].                                                   \n",
    "    dom = tiledb.Domain(\n",
    "        tiledb.Dim(name=\"rows\", domain=(1, 4), tile=2, dtype=np.int32),\n",
    "        tiledb.Dim(name=\"cols\", domain=(1, 4), tile=2, dtype=np.int32),\n",
    "    )\n",
    "\n",
    "    # The array will be dense with a single attribute \"a\" so each (i,j) cell can store an integer.                                  \n",
    "    schema = tiledb.ArraySchema(\n",
    "        domain=dom, sparse=False, attrs=[tiledb.Attr(name=\"a\", dtype=np.int32)]\n",
    "    )\n",
    "\n",
    "    # Create the (empty) array on disk.                                                                                             \n",
    "    tiledb.DenseArray.create(array_name, schema)\n",
    "    \n",
    "            # First write                                                                                                               \n",
    "        data = np.array(([1, 2], [3, 4]))\n",
    "        A[1:3, 1:3] = data\n",
    "\n",
    "        # Second write                                                                                                              \n",
    "        data = np.array(([5, 6, 7, 8], [9, 10, 11, 12]))\n",
    "        A[2:4, 1:5] = data\n",
    "\n",
    "\n",
    "def write_array():\n",
    "    # Open the array and write to it.                                                                                               \n",
    "    with tiledb.DenseArray(array_name, mode=\"w\") as A:\n",
    "        # First write                                                                                                               \n",
    "        data = np.array(([1, 2], [3, 4]))\n",
    "        A[1:3, 1:3] = data\n",
    "\n",
    "        # Second write                                                                                                              \n",
    "        data = np.array(([5, 6, 7, 8], [9, 10, 11, 12]))\n",
    "        A[2:4, 1:5] = data\n",
    "\n",
    "\n",
    "def read_array():\n",
    "    # Open the array and read from it.                                                                                              \n",
    "    with tiledb.DenseArray(array_name, mode=\"r\") as A:\n",
    "        # Slice the entire array                                                                                                    \n",
    "        data = A[:]\n",
    "        print(data[\"a\"])\n",
    "\n",
    "\n",
    "if tiledb.object_type(array_name) != \"array\":\n",
    "    create_array()\n",
    "    write_array()\n",
    "\n",
    "read_array()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "# array_name = 'sift_base'\n",
    "array_name = 's3://tiledb-lums/sift_base'\n",
    "tile_size = 100000\n",
    "block_size = 10000\n",
    "create_fvecs_array(filename, array_name, tile_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf660c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fvecs_array(filename, array_name, tile_size, block_size):\n",
    "    with tiledb.from_numpy(array_name, np.zeros((0,))) as A:\n",
    "        dim = None\n",
    "        offset = 0\n",
    "        for block in fvecs_generator(filename, block_size):\n",
    "            if dim is None:\n",
    "                dim = block.shape[1]\n",
    "                A.schema.set_domain((0, None), (0, dim))\n",
    "                A.schema.set_tile((tile_size, dim))\n",
    "                A.schema.set_cell_order(tiledb.Layout.ROW_MAJOR)\n",
    "                A.schema.set_sparse(False)\n",
    "                A.schema.set_attrs(tiledb.Attr(\"features\", dtype=np.float32, var=False))\n",
    "            num_rows = block.shape[0]\n",
    "            A[offset:offset+num_rows] = block\n",
    "            offset += num_rows\n",
    "    A.schema.set_capacity(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a1722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8475b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/Users/lums/TileDB/feature-vector-prototype/external/data/sift/sift_base.fvecs'\n",
    "block_size = 1000\n",
    "dimension = 128\n",
    "gen = fvecs_generator(filename, block_size, dimension)\n",
    "a = next(gen)\n",
    "print(f'a is {type(a)} with shape {a.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740927b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tiledb\n",
    "\n",
    "def fvecs_generator(filename, block_size):\n",
    "    f = open(filename, 'rb')\n",
    "    dim = np.fromfile(f, dtype=np.int32, count=1)[0]\n",
    "    chunk_size = block_size * dim\n",
    "    while True:\n",
    "        chunk = np.fromfile(f, dtype=np.float32, count=chunk_size)\n",
    "        if chunk.size == 0:\n",
    "            break\n",
    "        yield chunk.reshape((-1, dim))\n",
    "    f.close()\n",
    "\n",
    "def create_fvecs_array(filename, array_name, tile_size, block_size):\n",
    "    with tiledb.from_numpy(array_name, np.zeros((0,))) as A:\n",
    "        dim = None\n",
    "        offset = 0\n",
    "        for block in fvecs_generator(filename, block_size):\n",
    "            if dim is None:\n",
    "                dim = block.shape[1]\n",
    "                A.schema.set_domain((0, None), (0, dim))\n",
    "                A.schema.set_tile((tile_size, dim))\n",
    "                A.schema.set_cell_order(tiledb.Layout.ROW_MAJOR)\n",
    "                A.schema.set_sparse(False)\n",
    "                A.schema.set_attrs(tiledb.Attr(\"features\", dtype=np.float32, var=False))\n",
    "            num_rows = block.shape[0]\n",
    "            A[offset:offset+num_rows] = block\n",
    "            offset += num_rows\n",
    "    A.schema.set_capacity(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e8833",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'external/data/sift/sift_base.fvecs'\n",
    "array_name = 'sift_base'\n",
    "tile_size = 100000\n",
    "block_size = 10000\n",
    "create_fvecs_array(filename, array_name, tile_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb7519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
